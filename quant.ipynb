{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/quant/lib/python3.8/site-packages/torch/__init__.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25\n",
    "torch.manual_seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_dir = '~/.torch/data'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "img_datasets = {\n",
    "    'train': torchvision.datasets.CIFAR10(data_dir, train=True,\n",
    "                                          transform=data_transforms['train'], download=True),\n",
    "    'val': torchvision.datasets.CIFAR10(data_dir, train=False,\n",
    "                                          transform=data_transforms['val'], download=True)}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(img_datasets[x], batch_size=32,\n",
    "                                              shuffle=True, num_workers=32)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in dataloaders.keys()}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "  \"\"\"\n",
    "  Support function for model training.\n",
    "\n",
    "  Args:\n",
    "    model: Model to be trained\n",
    "    criterion: Optimization criterion (loss)\n",
    "    optimizer: Optimizer to use for training\n",
    "    scheduler: Instance of ``torch.optim.lr_scheduler``\n",
    "    num_epochs: Number of epochs\n",
    "    device: Device to run the training on. Must be 'cpu' or 'cuda'\n",
    "  \"\"\"\n",
    "  since = time.time()\n",
    "\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train()  # Set model to training mode\n",
    "      else:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "\n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "\n",
    "      # Iterate over data.\n",
    "      for inputs, labels in tqdm(dataloaders[phase]):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "          outputs = model(inputs)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # backward + optimize only if in training phase\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "      if phase == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "      epoch_loss = running_loss / dataset_sizes[phase]\n",
    "      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        phase, epoch_loss, epoch_acc))\n",
    "\n",
    "      # deep copy the model\n",
    "      if phase == 'val' and epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print()\n",
    "\n",
    "  time_elapsed = time.time() - since\n",
    "  print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "  print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "  # load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_float = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "for params in model_float.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "model_float.classifier[1] = nn.Linear(model_float.classifier[1].in_features, len(img_datasets['val'].classes))\n",
    "\n",
    "model_float = model_float.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_float.parameters(), lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.76it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 31.8268 Acc: 21.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:10<00:00, 30.77it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 26.7346 Acc: 23.3738\n",
      "\n",
      "Epoch 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:40<00:00, 38.98it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 30.8044 Acc: 22.4613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.02it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 31.3225 Acc: 22.6486\n",
      "\n",
      "Epoch 2/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:40<00:00, 38.76it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 30.9927 Acc: 22.5457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.83it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 26.1890 Acc: 23.6709\n",
      "\n",
      "Epoch 3/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.44it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 30.7973 Acc: 22.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.98it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 28.1808 Acc: 23.1789\n",
      "\n",
      "Epoch 4/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.10it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 30.8304 Acc: 22.6468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.65it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 24.3661 Acc: 24.2236\n",
      "\n",
      "Epoch 5/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.09it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 30.7493 Acc: 22.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.80it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 28.7463 Acc: 22.9681\n",
      "\n",
      "Epoch 6/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.60it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 31.1733 Acc: 22.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.63it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 25.9346 Acc: 23.8275\n",
      "\n",
      "Epoch 7/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.18it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 25.0437 Acc: 23.8714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.09it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 20.2349 Acc: 24.9808\n",
      "\n",
      "Epoch 8/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.58it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 23.9762 Acc: 23.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 34.06it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 19.9328 Acc: 24.9808\n",
      "\n",
      "Epoch 9/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:39<00:00, 39.55it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 23.6106 Acc: 23.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 19.3941 Acc: 25.2588\n",
      "\n",
      "Training complete in 8m 13s\n",
      "Best val Acc: 25.258786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_float = train_model(model_float, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model_float.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params[70:]:\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_float.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.29it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 13.7838 Acc: 27.4632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.17it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 8.1108 Acc: 29.2173\n",
      "\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.19it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 6.6823 Acc: 29.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.26it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.2202 Acc: 29.6550\n",
      "\n",
      "Epoch 2/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.29it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 4.1760 Acc: 30.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.51it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.6660 Acc: 29.5974\n",
      "\n",
      "Epoch 3/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.08it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.8644 Acc: 30.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.84it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.4314 Acc: 29.7252\n",
      "\n",
      "Epoch 4/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 27.98it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9112 Acc: 31.2994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.94it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.5025 Acc: 29.8946\n",
      "\n",
      "Epoch 5/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.14it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3783 Acc: 31.5138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.55it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.7162 Acc: 29.8978\n",
      "\n",
      "Epoch 6/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:55<00:00, 28.18it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1685 Acc: 31.5982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.85it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.3491 Acc: 29.9904\n",
      "\n",
      "Epoch 7/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:54<00:00, 28.44it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6166 Acc: 31.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.53it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.0850 Acc: 30.0447\n",
      "\n",
      "Epoch 8/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:53<00:00, 29.06it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4317 Acc: 31.8676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.66it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 7.0728 Acc: 30.0703\n",
      "\n",
      "Epoch 9/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:53<00:00, 29.40it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3486 Acc: 31.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.31it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9423 Acc: 30.1278\n",
      "\n",
      "Epoch 10/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:53<00:00, 29.16it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3330 Acc: 31.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.08it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9015 Acc: 30.1182\n",
      "\n",
      "Epoch 11/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:53<00:00, 29.26it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2680 Acc: 31.9283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 33.84it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9146 Acc: 30.1310\n",
      "\n",
      "Epoch 12/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:53<00:00, 29.00it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3022 Acc: 31.9136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:10<00:00, 31.23it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9332 Acc: 30.1342\n",
      "\n",
      "Epoch 13/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:54<00:00, 28.78it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2586 Acc: 31.9277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 32.01it/s]\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9605 Acc: 30.1502\n",
      "\n",
      "Epoch 14/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:54<00:00, 28.55it/s]\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2431 Acc: 31.9360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:09<00:00, 31.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 6.9848 Acc: 30.1278\n",
      "\n",
      "Training complete in 16m 5s\n",
      "Best val Acc: 30.150160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_float = train_model(model_float, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=15, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_float.state_dict(), \"mobilenetv2_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = MobileNetV2()\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_path = '~/.torch/data'\n",
    "saved_model_dir = './'\n",
    "float_model_file = 'mobilenetv2_cifar10.pth'\n",
    "scripted_float_model_file = 'mobilenet_quantization_scripted.pth'\n",
    "scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 50\n",
    "\n",
    "data_loader = dataloaders['train']\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(img_datasets['val'], batch_size=eval_batch_size, num_workers=32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n",
    "# while also improving numerical accuracy. While this can be used with any model, this is\n",
    "# especially common with quantized models.\n",
    "\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\n",
    "float_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 8.926889\n",
      "........................................................................................................................................................................................................Evaluation accuracy on 10000 images, 94.43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_eval_batches = 200\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Inverted Residual Block:After observer insertion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/quant/lib/python3.8/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/quant/lib/python3.8/site-packages/torch/quantization/observer.py:243: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.1202375590801239, zero_point=0, padding=(1, 1), groups=32)\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.18784166872501373, zero_point=63)\n",
      "  (2): Identity()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 2.360679\n",
      "........................................................................................................................................................................................................Evaluation accuracy on 10000 images, 48.33\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "myModel.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n",
    "\n",
    "# Calibrate with the training set\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/quant/lib/python3.8/site-packages/torch/quantization/observer.py:955: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................Evaluation accuracy on 10000 images, 80.12\n"
     ]
    }
   ],
   "source": [
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "per_channel_quantized_model.fuse_model()\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True)\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = load_model(saved_model_dir + float_model_file)\n",
    "qat_model.fuse_model()\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvBnReLU2d(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0])\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): ConvBn2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (weight_fake_quant): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0])\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    )\n",
      "    (activation_post_process): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Loss tensor(0.0801, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 97.031 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 0 :Evaluation accuracy on 10000 images, 91.78\n",
      "....................Loss tensor(0.1211, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 96.562 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 1 :Evaluation accuracy on 10000 images, 90.30\n",
      "....................Loss tensor(0.1353, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 96.094 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 2 :Evaluation accuracy on 10000 images, 89.76\n",
      "....................Loss tensor(0.2350, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 92.812 Acc@5 99.844\n",
      "........................................................................................................................................................................................................Epoch 3 :Evaluation accuracy on 10000 images, 89.31\n",
      "....................Loss tensor(0.1580, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 94.844 Acc@5 99.844\n",
      "........................................................................................................................................................................................................Epoch 4 :Evaluation accuracy on 10000 images, 90.72\n",
      "....................Loss tensor(0.1394, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 95.469 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 5 :Evaluation accuracy on 10000 images, 91.10\n",
      "....................Loss tensor(0.1153, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 95.469 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 6 :Evaluation accuracy on 10000 images, 91.37\n",
      "....................Loss tensor(0.0730, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 97.812 Acc@5 100.000\n",
      "........................................................................................................................................................................................................Epoch 7 :Evaluation accuracy on 10000 images, 91.29\n"
     ]
    }
   ],
   "source": [
    "num_train_batches = 20\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(8):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
    "    quantized_model.eval()\n",
    "    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 1\n",
      "\tat::get_num_interop_threads() : 44\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 1\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 1\n",
      "Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
      "std::thread::hardware_concurrency() : 88\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "print(torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  51 ms\n",
      "Elapsed time: 107 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.741507053375244"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model=torch.jit.load(saved_model_dir + scripted_float_model_file)\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "               model_inference        74.04%     350.883ms        99.99%     473.885ms     473.885ms             1  \n",
      "                       forward         0.29%       1.360ms        25.76%     122.080ms     122.080ms             1  \n",
      "                  aten::conv2d         0.08%     392.741us        20.37%      96.558ms       1.857ms            52  \n",
      "             aten::convolution         0.06%     297.117us        20.29%      96.165ms       1.849ms            52  \n",
      "            aten::_convolution         0.11%     503.328us        20.23%      95.868ms       1.844ms            52  \n",
      "    aten::_convolution_nogroup         0.05%     237.275us        11.64%      55.169ms       1.623ms            34  \n",
      "             aten::thnn_conv2d         0.03%     138.802us        11.58%      54.886ms       1.614ms            34  \n",
      "     aten::thnn_conv2d_forward         0.80%       3.810ms        11.55%      54.748ms       1.610ms            34  \n",
      "      aten::mkldnn_convolution         8.45%      40.061ms         8.48%      40.195ms       2.233ms            18  \n",
      "                  aten::addmm_         6.51%      30.835ms         6.51%      30.835ms     181.385us           170  \n",
      "                    aten::relu         0.12%     572.580us         4.81%      22.785ms     651.000us            35  \n",
      "               aten::threshold         4.65%      22.019ms         4.69%      22.212ms     634.641us            35  \n",
      "                   aten::copy_         3.16%      14.965ms         3.16%      14.965ms      87.005us           172  \n",
      "                  aten::select         0.37%       1.746ms         0.45%       2.117ms       4.151us           510  \n",
      "               aten::unsqueeze         0.21%     978.911us         0.28%       1.314ms       2.577us           510  \n",
      "                    aten::view         0.25%       1.176ms         0.25%       1.176ms       2.034us           578  \n",
      "                     aten::add         0.23%       1.082ms         0.23%       1.082ms     108.213us            10  \n",
      "              aten::as_strided         0.15%     710.509us         0.15%     710.509us       0.695us          1023  \n",
      "                   aten::empty         0.13%     609.377us         0.13%     609.377us       1.643us           371  \n",
      "                 aten::reshape         0.07%     349.639us         0.11%     519.225us       3.054us           170  \n",
      "                    aten::set_         0.08%     391.693us         0.08%     391.693us       3.695us           106  \n",
      "                      defaults         0.06%     298.378us         0.06%     298.378us       1.073us           278  \n",
      "                    aten::mean         0.00%      20.213us         0.05%     224.353us     224.353us             1  \n",
      "                     aten::sum         0.03%     152.363us         0.03%     164.343us     164.343us             1  \n",
      "                 aten::resize_         0.02%      91.020us         0.02%      91.020us       2.677us            34  \n",
      "                  aten::linear         0.00%      12.424us         0.01%      69.019us      69.019us             1  \n",
      "       aten::_nnpack_available         0.01%      45.754us         0.01%      45.754us       1.346us            34  \n",
      "             aten::as_strided_         0.01%      39.987us         0.01%      39.987us       2.221us            18  \n",
      "                    aten::div_         0.00%      23.015us         0.01%      39.797us      39.797us             1  \n",
      "                   aten::addmm         0.01%      29.383us         0.01%      38.444us      38.444us             1  \n",
      "                   aten::zeros         0.00%      13.963us         0.01%      26.042us      26.042us             1  \n",
      "                  aten::detach         0.00%      21.278us         0.00%      21.278us       0.626us            34  \n",
      "                       aten::t         0.00%      13.374us         0.00%      18.151us      18.151us             1  \n",
      "                      aten::to         0.00%       9.276us         0.00%      16.782us      16.782us             1  \n",
      "                   aten::fill_         0.00%       8.716us         0.00%       8.716us       8.716us             1  \n",
      "               aten::transpose         0.00%       2.819us         0.00%       4.777us       4.777us             1  \n",
      "                  aten::expand         0.00%       3.441us         0.00%       4.282us       4.282us             1  \n",
      "                   aten::zero_         0.00%       2.707us         0.00%       2.707us       2.707us             1  \n",
      "           aten::empty_strided         0.00%       2.396us         0.00%       2.396us       2.396us             1  \n",
      "                 aten::dropout         0.00%       2.028us         0.00%       2.028us       2.028us             1  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 473.911ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              model_inference        94.72%       12.005s       100.00%       12.674s       12.674s             1  \n",
      "                                 __setstate__         0.01%       1.284ms         4.78%     605.667ms      11.428ms            53  \n",
      "                              set_weight_bias         0.00%     528.731us         4.77%     604.383ms      11.403ms            53  \n",
      "                    quantized::conv2d_prepack         2.11%     267.058ms         4.76%     603.508ms      11.606ms            52  \n",
      "                                 aten::select         0.93%     118.037ms         1.10%     138.807ms       4.067us         34132  \n",
      "                                   aten::item         0.35%      44.735ms         0.78%      98.800ms       2.894us         34134  \n",
      "              aten::q_per_channel_zero_points         0.54%      68.162ms         0.54%      68.162ms       3.994us         17066  \n",
      "                                      forward         0.01%       1.498ms         0.49%      61.740ms      61.740ms             1  \n",
      "                    aten::_local_scalar_dense         0.43%      54.066ms         0.43%      54.066ms       1.584us         34134  \n",
      "                       quantized::conv2d_relu         0.34%      42.728ms         0.36%      46.100ms       1.317ms            35  \n",
      "                   aten::q_per_channel_scales         0.24%      30.590ms         0.24%      30.590ms       1.792us         17066  \n",
      "                             aten::as_strided         0.16%      20.773ms         0.16%      20.773ms       0.609us         34133  \n",
      "                            quantized::conv2d         0.10%      12.137ms         0.10%      12.283ms     722.540us            17  \n",
      "                             aten::contiguous         0.00%      10.874us         0.02%       3.036ms       3.036ms             1  \n",
      "                                  aten::copy_         0.02%       2.996ms         0.02%       2.999ms       1.500ms             2  \n",
      "                                   aten::set_         0.01%       1.083ms         0.01%       1.083ms       5.062us           214  \n",
      "                               quantized::add         0.00%     607.459us         0.01%     803.099us      80.310us            10  \n",
      "    aten::_empty_per_channel_affine_quantized         0.00%     494.265us         0.00%     559.952us      10.565us            53  \n",
      "                    aten::quantize_per_tensor         0.00%     537.419us         0.00%     537.419us     268.710us             2  \n",
      "                                  aten::empty         0.00%     404.092us         0.00%     404.092us       1.063us           380  \n",
      "                aten::_empty_affine_quantized         0.00%     403.039us         0.00%     403.039us       3.445us           117  \n",
      "                                   aten::mean         0.00%      28.060us         0.00%     389.534us     389.534us             1  \n",
      "                    quantized::linear_prepack         0.00%     158.284us         0.00%     346.051us     346.051us             1  \n",
      "                                aten::qscheme         0.00%     235.213us         0.00%     235.213us       2.735us            86  \n",
      "                                aten::q_scale         0.00%     180.224us         0.00%     180.224us       2.120us            85  \n",
      "                           aten::q_zero_point         0.00%     179.678us         0.00%     179.678us       2.114us            85  \n",
      "                             aten::dequantize         0.00%     171.384us         0.00%     177.774us      88.887us             2  \n",
      "                                    aten::sum         0.00%     122.139us         0.00%     137.496us     137.496us             1  \n",
      "                     aten::q_per_channel_axis         0.00%     130.843us         0.00%     130.843us       2.516us            52  \n",
      "                            quantized::linear         0.00%      96.541us         0.00%     103.087us     103.087us             1  \n",
      "                                     aten::to         0.00%      76.749us         0.00%      90.358us       0.844us           107  \n",
      "                                   aten::div_         0.00%      22.067us         0.00%      46.738us      46.738us             1  \n",
      "                             aten::empty_like         0.00%      19.009us         0.00%      35.702us      35.702us             1  \n",
      "                                  aten::zeros         0.00%      21.984us         0.00%      33.663us      33.663us             1  \n",
      "                                     defaults         0.00%      13.397us         0.00%      13.397us       3.349us             4  \n",
      "                                  aten::fill_         0.00%      10.314us         0.00%      10.314us      10.314us             1  \n",
      "                          aten::empty_strided         0.00%       3.569us         0.00%       3.569us       3.569us             1  \n",
      "                                aten::dropout         0.00%       2.461us         0.00%       2.461us       2.461us             1  \n",
      "                                  aten::zero_         0.00%       2.279us         0.00%       2.279us       2.279us             1  \n",
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.674s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model=torch.jit.load(saved_model_dir + scripted_quantized_model_file)\n",
    "        model(inputs)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `../' from member names\n",
      "tar: ../z_quant: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar czf quant.tar.gz ../z_quant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch 1.8.1 (quant)",
   "language": "python",
   "name": "quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
